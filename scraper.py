import time
import os
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# --- üìÅ –ü–™–¢ –ö–™–ú –ü–ê–ü–ö–ò–¢–ï (GITHUB ACTIONS MODE) ---
# –í –æ–±–ª–∞–∫–∞ —Å–º–µ, –ø—Ä–∞–≤–∏–º –ø–∞–ø–∫–∞ 'data' –ø—Ä–∏ —Å–∫—Ä–∏–ø—Ç–∞
output_dir = os.path.join(os.getcwd(), "data")

if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print("üìÅ –ü–∞–ø–∫–∞—Ç–∞ 'data' –µ –≥–æ—Ç–æ–≤–∞. Let's cook.")

output_filename = os.path.join(output_dir, "lekaribg_full_data.xlsx")
print(f"üéØ –î–∞–Ω–Ω–∏—Ç–µ –æ—Ç–∏–≤–∞—Ç —Ç—É–∫: {output_filename}")

# --- ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò –ù–ê –ë–†–ê–£–ó–™–†–ê ---
chrome_options = Options()
chrome_options.add_argument("--headless")  # –ë–µ–∑ –ø—Ä–æ–∑–æ—Ä–µ—Ü (–∑–∞–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–æ –∑–∞ —Å—ä—Ä–≤—ä—Ä–∞)
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--window-size=1920,1080")
chrome_options.add_argument('--log-level=3')
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

# --- üöó –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê –î–†–ê–ô–í–™–†–ß–û–í–¶–ò ---
print("‚è≥ –ü–∞–ª—è –≥—É–º–∏—Ç–µ –Ω–∞ Chrome...")
try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    print("‚úÖ –î—Ä–∞–π–≤—ä—Ä—ä—Ç –∑–∞—Ä–µ–¥–∏. Skibidi bop yes yes.")
except Exception as e:
    print(f"üí• –ú–∞–º–∫–∞ –º—É —á–æ–≤–µ—á–µ, –¥—Ä–∞–π–≤—ä—Ä—ä—Ç –≥—Ä—ä–º–Ω–∞: {e}")
    exit(1)

# --- üíæ –ó–ê–ü–ò–°–í–ê–ß–ö–ê–¢–ê ---
def save_single_record(record):
    if not record: return
    try:
        new_df = pd.DataFrame([record])
        
        if os.path.exists(output_filename):
            try:
                existing_df = pd.read_excel(output_filename)
                final_df = pd.concat([existing_df, new_df], ignore_index=True)
            except:
                time.sleep(1)
                existing_df = pd.read_excel(output_filename)
                final_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            final_df = new_df

        final_df.to_excel(output_filename, index=False)
        print(f"üíæ {record.get('–ò–º–µ', 'N/A')} –∑–∞–ø–∏—Å–∞–Ω.")
    except Exception as e:
        print(f"‚ùå Save Error: {e}")

# --- üïµÔ∏è‚Äç‚ôÇÔ∏è PROFILE SCRAPER ---
def scrape_details_from_profile(url, basic_info):
    print(f"   üëâ Visiting: {url}")
    try:
        driver.get(url)
        # –ù–∞–º–∞–ª–∏—Ö –º–∞–ª–∫–æ –≤—Ä–µ–º–µ—Ç–æ, –∑–∞ –¥–∞ –≤—ä—Ä–≤–∏ –ø–æ-–±—ä—Ä–∑–æ –≤ GitHub, –Ω–æ –Ω–µ –ø—Ä–µ–∫–∞–ª–µ–Ω–æ
        time.sleep(0.5) 
        
        try:
            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        except:
            return basic_info

        # –ò–º–µ
        try:
            full_name = driver.find_element(By.XPATH, "//h1//span[@itemprop='name']").text.strip()
            basic_info["–ò–º–µ"] = full_name
        except: pass

        # –¢–∞–±–ª–∏—Ü–∞ —Å –¥–∞–Ω–Ω–∏
        try:
            table = driver.find_element(By.ID, "TableCustomFieldsBig")
            rows = table.find_elements(By.TAG_NAME, "tr")
            
            for row in rows:
                try:
                    th = row.find_element(By.TAG_NAME, "th").text.strip()
                    td = row.find_element(By.TAG_NAME, "td").text.strip()
                    
                    if "–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ" in th:
                        basic_info["–†–∞–±–æ—Ç–Ω–æ –≤—Ä–µ–º–µ"] = td
                    elif "–¢–µ–ª–µ—Ñ–æ–Ω" in th:
                        basic_info["–¢–µ–ª–µ—Ñ–æ–Ω"] = td
                    elif "–ê–¥—Ä–µ—Å" in th:
                        basic_info["–ê–¥—Ä–µ—Å"] = td
                    elif "–°–ø–µ—Ü–∏–∞–ª–Ω–æ—Å—Ç" in th:
                        basic_info["–°–ø–µ—Ü–∏–∞–ª–Ω–æ—Å—Ç"] = td
                except: continue
        except: pass

        basic_info["Last Updated"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        return basic_info

    except Exception as e:
        print(f"üíÄ Profile Error: {e}")
        return basic_info

# --- üìú MAIN LOOP (INFINITE GRIND) ---
page = 1
# –í–ù–ò–ú–ê–ù–ò–ï: –ú–∞—Ö–Ω–∞—Ö–º–µ max_pages. –¶–∏–∫—ä–ª—ä—Ç –µ –±–µ–∑–∫—Ä–∞–µ–Ω, –¥–æ–∫–∞—Ç–æ –Ω–µ —Å–ø—Ä–µ –¥–∞ –Ω–∞–º–∏—Ä–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏.

print("üöÄ Start the INFINITE grind...")

try:
    while True:
        target_url = f"https://lekaribg.net/listing-category/lekari/page/{page}/"
        print(f"\nüìÑ --- PAGE {page} ---")
        driver.get(target_url)
        
        try:
            # –ß–∞–∫–∞–º–µ –∑–∞ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –∏–ª–∏ —Å—ä–æ–±—â–µ–Ω–∏–µ –∑–∞ –≥—Ä–µ—à–∫–∞
            try:
                WebDriverWait(driver, 5).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, ".wlt_search_results"))
                )
            except:
                # –ê–∫–æ –Ω—è–º–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ —Å–ª–µ–¥ 5 —Å–µ–∫—É–Ω–¥–∏, –∑–Ω–∞—á–∏ —Å–º–µ —Å—Ç–∏–≥–Ω–∞–ª–∏ –∫—Ä–∞—è
                print("‚õî –ù—è–º–∞ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å —Ä–µ–∑—É–ª—Ç–∞—Ç–∏. –í–µ—Ä–æ—è—Ç–Ω–æ –∫—Ä–∞–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∏—Ç–µ.")
                break

            items = driver.find_elements(By.CSS_SELECTOR, ".wlt_search_results .itemdata")
            
            # –í–¢–û–†–ê –ü–†–û–í–ï–†–ö–ê: –ê–∫–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ä—Ç –≥–æ –∏–º–∞, –Ω–æ –µ –ø—Ä–∞–∑–µ–Ω
            if not items:
                print("‚õî –ù–∞–º–µ—Ä–∏—Ö 0 —Ä–µ–∑—É–ª—Ç–∞—Ç–∞. Game Over. –§–∏–Ω–∏—Ç–æ.")
                break

            print(f"üîé –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞ –∏–º–∞ {len(items)} –¥–æ–∫—Ç–æ—Ä–∏.")
            
            doctors_on_page = []
            for item in items:
                try:
                    link_el = item.find_element(By.CSS_SELECTOR, "h4 a")
                    name = link_el.text.strip()
                    url = link_el.get_attribute("href")
                    
                    phone_backup = "-"
                    try:
                        phone_backup = item.find_element(By.CSS_SELECTOR, ".wlt_shortcode_phone").text.strip()
                    except: pass

                    doc_data = {
                        "–ò–º–µ": name,
                        "URL": url,
                        "–¢–µ–ª–µ—Ñ–æ–Ω": phone_backup
                    }
                    doctors_on_page.append(doc_data)
                except: continue

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ —Å–ø–∏—Å—ä–∫–∞
            for doc in doctors_on_page:
                full_data = scrape_details_from_profile(doc['URL'], doc)
                save_single_record(full_data)

            page += 1
            
        except Exception as e:
            print(f"ü§¨ CRITICAL ERROR –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {e}")
            # –ê–∫–æ –≥—Ä—ä–º–Ω–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞, –ø—Ä–æ–±–≤–∞–º–µ —Å–ª–µ–¥–≤–∞—â–∞—Ç–∞ –∑–∞ –≤—Å–µ–∫–∏ —Å–ª—É—á–∞–π, –∏–ª–∏ —Å–ø–∏—Ä–∞–º–µ
            # –ó–∞ –¥–∞ —Å–º–µ —Å–∏–≥—É—Ä–Ω–∏, —á–µ –Ω—è–º–∞ –¥–∞ –∑–∞—Ü–∏–∫–ª–∏, —É–≤–µ–ª–∏—á–∞–≤–∞–º–µ –±—Ä–æ—è—á–∞
            page += 1
            if page > 500: # Hard limit, –¥–∞ –Ω–µ –≥—Ä—ä–º–Ω–µ —Å—ä—Ä–≤—ä—Ä–∞ –Ω–∞ GitHub –∞–∫–æ –Ω–µ—â–æ —Å–µ –æ–±—ä—Ä–∫–∞ –±—Ä—É—Ç–∞–ª–Ω–æ
                print("üíÄ Hard limit reached (500 pages). Stopping safety protocol.")
                break
            continue

finally:
    try:
        driver.quit()
    except: pass
    print(f"\nüèÅ –í—Å–∏—á–∫–æ –ø—Ä–∏–∫–ª—é—á–∏. –î–∞–Ω–Ω–∏—Ç–µ —Å–∞ –≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∏—Ç–µ.")
